{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo1: Role Keywords Extraction (抽取角色关键词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English\n",
      "Loading word vectors......\n"
     ]
    }
   ],
   "source": [
    "from keywords_extractor_w import KeywordsExtractor\n",
    "KE = KeywordsExtractor(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/negative2015.csv')\n",
    "contents = list(data['Text'])\n",
    "labels = list(data['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 16662.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Label-Similarity for label: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 545.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Label-Similarity for label: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [00:02<00:00, 362.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at saved_keywords/global_ls_dict_negative2015.pkl\n",
      "counting #doc for each word...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 19739.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Label-Correlation for label: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [00:00<00:00, 478829.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing Label-Correlation for label: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [00:00<00:00, 538586.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved at saved_keywords/global_lr_dict_negative2015.pkl\n",
      "First level keys:  ['neutral', 'negative']\n",
      "Second level keys:  ['lr', 'ls', 'ccw', 'scw', 'fcw', 'iw']\n",
      "already saved at saved_keywords/global_kws_dict_negative2015.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['global_ls', 'global_lr', 'global_roles'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract keywords\n",
    "kws_dict = KE.global_role_kws_extraction_one_line(contents, labels, output_dir='saved_keywords',name='negative2015')\n",
    "kws_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords for \"neutral\":\n",
      "ccw: ['lowered', 'split', 'mediorce', 'beat', 'generally', 'casual', 'simple', 'settled', 'per', 'depending']\n",
      "scw: ['rather', 'overpriced', 'price', 'back', 'best', 'away', 'would', 'one']\n",
      "fcw: ['eaten', 'trip', 'companion', 'huge', 'village', 'son', 'waiter', 'pancakes', 'five', 'requests']\n",
      "iw: ['restaurant', 'make', 'restaurants', 'worth']\n",
      "keywords for \"negative\":\n",
      "ccw: ['bad', 'unhelpful', 'unfriendly', 'horrible', 'terrible', 'wrong', 'lousy', 'worst', 'dissapointing', 'miserable']\n",
      "scw: ['good', 'mediocre', 'kind', 'kanish', 'however', 'poor', 'prixe', 'okay', 'guess', 'expect']\n",
      "fcw: ['filling', 'hampton', 'absurdly', 'penne', 'made', 'cake', 'selection', 'sauce', 'ingredients', 'gig']\n",
      "iw: ['makes', 'dumplings', 'watering', 'downtown', 'fix', 'sashimi', 'list', 'italian', 'crammed', 'potato']\n"
     ]
    }
   ],
   "source": [
    "for key in kws_dict['global_roles']:\n",
    "    print(f\"keywords for \\\"{key}\\\":\")\n",
    "    for each in ['ccw','scw','fcw','iw']:\n",
    "        print(f\"{each}: {kws_dict['global_roles'][key][each][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lr', 'ls', 'ccw', 'scw', 'fcw', 'iw'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo2: Selective Text Augmentation (针对性文本增强)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: English\n"
     ]
    }
   ],
   "source": [
    "from text_augmenter import TextAugmenter\n",
    "TA = TextAugmenter(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在`TextAugmenter`类中，对删除、替换、插入、顺序互换等增强操作(operations)做了统一的接口:\n",
    "- .aug_by_deletion(text, p, mode, selected_words)\n",
    "- .aug_by_replacement(text, p, mode, selected_words)\n",
    "- .aug_by_insertion(text, p, mode, selected_words)\n",
    "- .aug_by_swap(text, p, mode, selected_words)\n",
    "- .aug_by_selection(text, selected_words)\n",
    "\n",
    "上述5中方法中，除了`aug_by_selection()`之外，其余方法均可通过设置`mode='random'`或者`mode='selective'`来决定使用“随机”增强还是“针对性”增强。\n",
    "\n",
    "## 当使用随机增强时 (`mode='random'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the last two times i ordered from here my food was soo spicy that i could barely eat it and the $t$ took away from the flavor of the dish'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the last two times ordered food was soo spicy that i could barely eat and the $ t $ took away from flavor dish\n",
      "the last two times i Judge_Clifford_Cretan from here my nutritious_foods was soo garlicky that i could barely eat it and the $ t $ relinquished away from the tart_flavor of the roast_suckling_pig\n",
      "pathetically the last two times i ordered from here my food was soo spicy that i could barely eat feebly million into it two and the $ t $ took away from the flavor outside of the dish\n",
      "the last two times the ordered from of my food was that spicy and i could soo eat it barely i $ the $ took away from the flavor here t dish\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the last two times i ordered from here my food was soo spicy that i could barely eat it and the $t$ took away from the flavor of the dish\"\n",
    "p = 0.2\n",
    "print(' '.join(TA.aug_by_deletion(text=sentence,p=p,mode='random')))\n",
    "print(' '.join(TA.aug_by_replacement(text=sentence,p=p,mode='random')))\n",
    "print(' '.join(TA.aug_by_insertion(text=sentence,p=p,mode='random')))\n",
    "print(' '.join(TA.aug_by_swap(text=sentence,p=p,mode='random')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 当使用针对性增强时 (`mode='selective'`)\n",
    "跟随机增强相比，针对性增强只需要指定对应的`selected_words`即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is always cooked to perfection , the service is excellent\n",
      "Everything is always cooked to perfection , the service is unrivaled\n",
      "Everything is always cooked to perfection , great the service is excellent\n",
      "Everything is always cooked to perfection excellent the service is ,\n",
      "Everything cooked\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(TA.aug_by_deletion(text=sentence,p=p,mode='selective',selected_words=['Everything','excellent'])))\n",
    "print(' '.join(TA.aug_by_replacement(text=sentence,p=p,mode='selective',selected_words=['service','excellent'])))\n",
    "print(' '.join(TA.aug_by_insertion(text=sentence,p=p,mode='selective',selected_words=['service','excellent'])))\n",
    "print(' '.join(TA.aug_by_swap(text=sentence,p=p,mode='selective',selected_words=['service','excellent'])))\n",
    "print(' '.join(TA.aug_by_selection(text=sentence, selected_words=['Everything','cooked'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在文本分类任务中，不同的词可能会有不同的角色(roles)。在我们的论文中，我们提出如下规则：\n",
    "- 对于 deletion/replacement 操作，应避开 gold words\n",
    "- 对于 insertion 操作，应避开 venture words\n",
    "- 对于 selection 操作，直接选取 gold words 和标点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved keywords\n",
    "import pickle\n",
    "name = 'negative2015_w'\n",
    "global_kws_dict_path = f'saved_keywords/global_kws_dict_{name}.pkl'\n",
    "with open(global_kws_dict_path, 'rb') as f:\n",
    "    global_kws_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3801193/2798137568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_kws_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print(' '.join(TA.aug_by_deletion(sentence, p, 'selective', print_info=True,\n\u001b[0;32m----> 4\u001b[0;31m                    selected_words=kws['scw']+kws['fcw']+kws['iw'])))  # except ccw\n\u001b[0m\u001b[1;32m      5\u001b[0m print(' '.join(TA.aug_by_replacement(sentence, p, 'selective', print_info=True,\n\u001b[1;32m      6\u001b[0m                    selected_words=kws['scw']+kws['fcw'])))  # except ccw\n",
      "\u001b[0;32m~/STA/text_augmenter.py\u001b[0m in \u001b[0;36maug_by_deletion\u001b[0;34m(self, text, p, mode, selected_words, print_info)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 针对性删除难以控制数量，所以这里就控制一个上限吧\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# selected_words不一定都是本文中的，可能是给定的一个大集合，所以我们要先筛选出在本文中的词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mselected_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "category = 'negative'\n",
    "kws = global_kws_dict[category]\n",
    "print(' '.join(TA.aug_by_deletion(sentence, p, 'selective', print_info=True,\n",
    "                   selected_words=kws['scw']+kws['fcw']+kws['iw'])))  # except ccw\n",
    "print(' '.join(TA.aug_by_replacement(sentence, p, 'selective', print_info=True,\n",
    "                   selected_words=kws['scw']+kws['fcw'])))  # except ccw\n",
    "print(' '.join(TA.aug_by_insertion(sentence, p, 'selective', print_info=True,\n",
    "                   selected_words=kws['ccw']+kws['fcw'])))  # except fcw\n",
    "print(' '.join(TA.aug_by_swap(sentence, p, 'selective', print_info=True,\n",
    "                   selected_words=kws['iw'])))  \n",
    "\n",
    "punc_list = [w for w in ',.，。!?！？;；、']\n",
    "print(' '.join(TA.aug_by_selection(sentence, print_info=True,\n",
    "                    selected_words=kws['ccw']+punc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('bda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d50a08b76e8db6d4e3ad0ea54fb128acb154f960160ffb5e94410c5ec0ed5f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
